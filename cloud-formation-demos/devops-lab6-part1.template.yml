AWSTemplateFormatVersion: 2010-09-09

#  Rough approximation of AWS DevOps Lab 5 - Deploying the MadLibs application as 3 container-based apps running in ECS behind a load balancer.
#  Pretty cool when done, and a lot better than the original lab instructions.
#  Instructions:  
#    1.  Run this 'base' template.  It establishes roles, policies, security group, and ALB,  Use any VPC you like, default is fine.
#    2.  See the accompanying "2-services" template.  Follow the instructions within, running it three times to establish the three services.
#    3.  Be sure to delete the services when done, each runs in Fargate generating Fargate costs.  Delete this when done because of the ALB charges.

Parameters:

  CodePipelineBucketPrefix:
    Description: CodePipeline needs a utility bucket for its internal use.  Specify the prefix for the bucket name.  
    Type: String
    Default: codepipeline-kk-

  ECSCluster:
    Description:  The ECS Cluster that is ready to run our service / task definition.
    Type: String    
    Default: default

  VPC:
    Type: AWS::EC2::VPC::Id
    Description: Select a VPC, one with public subnets

  SubnetIds:
    Type: List<AWS::EC2::Subnet::Id>
    Description: Select 1-2 subnets in your selected VPC, private preferred if NATting is setup.  Fargate ENIs will connect here.

Resources:

  # This Role allows CodeBuild to do certain things on our behalf.
  # See the policy for the interesting stuff:
  CodeBuildRole:
    Type: AWS::IAM::Role
    Properties: 
      RoleName: !Sub ${AWS::StackName}-CodeBuildRole
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement: 
          Effect: Allow
          Principal:
            Service: codebuild.amazonaws.com
          Action: sts:AssumeRole

  # This Policy is attached to the CodeBuildRole.
  # CodeBuild is get and put on S3, CodeBuild, and CloudWatch Logs.  Allowed to login and push to ECR.  This all could probably be tightened quite a bit.
  CodeBuildPolicy:
    Type: AWS::IAM::Policy
    Properties: 
      PolicyName: !Sub ${AWS::StackName}-CodeBuildPolicy
      PolicyDocument: 
        Version: 2012-10-17
        Statement: 
          Effect: Allow
          Action: 
            - logs:CreateLogGroup
            - logs:CreateLogStream
            - logs:PutLogEvents
            - s3:putObject
            - s3:getObject
            - codebuild:*
            - ecr:Get*                    # For Docker builds pushing to ECR, one will need to GetAuthorizationToken
            - ecr:InitiateLayerUpload     # For Docker push to ECR
            - ecr:Upload*                 # For Docker push to ECR
            - ecr:Complete*               # For Docker push to ECR
            - ecr:*                       # I'm getting weird results on Docker push, and this fixed it.  TODO - Figure out what ECR permissions are needed.
          Resource: "*"
      Roles: 
        -  !Ref CodeBuildRole

  # This Role allows CodePipeline to make certain things on our behalf:
  # See the policy for the interesting stuff:
  CodePipelineRole:
    Type: AWS::IAM::Role
    Properties: 
      RoleName: !Sub ${AWS::StackName}-CodePipelineRole
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement: 
          Effect: Allow
          Principal:
            Service: codepipeline.amazonaws.com
          Action: sts:AssumeRole

  # This Policy is attached to the CodePipelineRole.
  # CodePipeline is allowed carte blanche on S3, CodeBuild, and CloudWatch Logs; could probably be tightened quite a bit.
  CodePipelinePolicy:
    Type: AWS::IAM::Policy
    Properties: 
      PolicyName: !Sub ${AWS::StackName}-CodePipelinePolicy
      PolicyDocument: 
        Version: 2012-10-17
        Statement: 
          Effect: Allow
          # I can't quite determine which S3 permission CodePipeline wants.  The one-click policy grants everything...
          # codebuild probably does not need to be wide open like this, and the logs should only need
          # to create the stream, group, and log events.
          # Ultimately I ran into too many permission errors with little information available in the documentation to debug, so I had to use "*".
          Action: 
            # - logs:CreateLogGroup
            # - logs:CreateLogStream
            # - logs:PutLogEvents
            # - s3:putObject
            # - s3:getObject
            # - codebuild:*
            # - elasticbeanstalk:*
            - "*"                             #  TODO - FIND OUT WHAT CODE PIPELINE permissions are needed.
          Resource: 
            - "*"
      Roles: 
        -  !Ref CodePipelineRole

  # This Role allows the ECS Task to write to CloudWatchLogs:
  ExecutionRole:
    Type: AWS::IAM::Role
    Properties: 
      RoleName: !Sub ${AWS::StackName}-ExecutionRole
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement: 
          Effect: Allow
          Principal:
            Service: ecs-tasks.amazonaws.com
          Action: sts:AssumeRole

  # This Policy is attached to the ExecutionRole.
  # Task is allowed to write to CloudWatch logs
  # "Fargate requires task definition to have execution role ARN to support log driver awslogs."
  ExecutionPolicy:
    Type: AWS::IAM::Policy
    Properties: 
      PolicyName: !Sub ${AWS::StackName}-ExecutionPolicy
      PolicyDocument: 
        Version: 2012-10-17
        Statement: 
          Effect: Allow
          Action:
            - logs:CreateLogGroup
            - logs:CreateLogStream
            - logs:PutLogEvents
            - ecr:Get*
            - ecr:BatchCheck*
            - ecr:BatchGet*
          Resource: 
            - "*"
      Roles: 
        -  !Ref ExecutionRole

  # This Role allows the containers within the ECS Task to make AWS API calls:
  TaskRole:
    Type: AWS::IAM::Role
    Properties: 
      RoleName: !Sub ${AWS::StackName}-TaskRole
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement: 
          Effect: Allow
          Principal:
            Service: ecs-tasks.amazonaws.com
          Action: sts:AssumeRole

  # This Policy is attached to the TaskRole.
  # Task is allowed to write to DynamoDB
  TaskPolicy:
    Type: AWS::IAM::Policy
    Properties: 
      PolicyName: !Sub ${AWS::StackName}-TaskPolicy
      PolicyDocument: 
        Version: 2012-10-17
        Statement: 
          Effect: Allow
          Action:
            - dynamodb:PutItem
          Resource: 
            - !GetAtt NounsVerbsAdjectivesTable.Arn
      Roles: 
        -  !Ref TaskRole

  NounsVerbsAdjectivesTable:
    Type: AWS::DynamoDB::Table
    Properties: 
      TableName: nouns_verbs_and_adjectives
      KeySchema: 
        - AttributeName: guid_str
          KeyType: HASH
      AttributeDefinitions: 
        - AttributeName: guid_str
          AttributeType: S
      ProvisionedThroughput: 
        ReadCapacityUnits: 5
        WriteCapacityUnits: 5

  # General Bucket where CodePipeline will store things:
  # Warning: This will need to be deleted manually before you can delete the stack.
  S3:
    Type: AWS::S3::Bucket
    Properties: 
      BucketName: !Sub ${CodePipelineBucketPrefix}-${AWS::Region}


  # Security Group for the task, which is assigned to the service (go figure).:
  ContainerSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      VpcId: !Ref VPC
      GroupDescription: SG for task
      Tags:
      - Key: Name
        Value: ContainerSecurityGroup
      SecurityGroupIngress:
      - IpProtocol: tcp
        FromPort: 80
        ToPort: 82
        CidrIp: 0.0.0.0/0


  # Now the Load Balancer, with all of its sub-components:
  ALB:
    Type: AWS::ElasticLoadBalancingV2::LoadBalancer
    Properties:
      Name: ALB
      Scheme: internet-facing
      Subnets: !Ref SubnetIds
      SecurityGroups: [!Ref ContainerSecurityGroup]
  # Listen on port 80, pass all traffic to WEBSITE target group:
  ALBListener:
    Type: AWS::ElasticLoadBalancingV2::Listener
    Properties:
      Port: 80
      Protocol: HTTP
      LoadBalancerArn: !Ref ALB
      DefaultActions:  # ALL ALBListeners must have a default, but we never want it to be invoked.
      - Type: fixed-response
        FixedResponseConfig:
          ContentType: text/plain
          MessageBody: Apparently, no other Listener Rules have been setup on this ALB
          StatusCode: 503

  # # This TargetGroup provides a default for the ALB until we add real mappings.
  # ALBDefaultTargetGroup:
  #   Type: AWS::ElasticLoadBalancingV2::TargetGroup
  #   Properties:
  #     Name: ALBTargetGroup
  #     VpcId: !Ref VPC
  #     Port: 80
  #     Protocol: HTTP
  #     TargetType: ip   # Necessary when the Tasks use awsvpc network type.  
  #     HealthCheckProtocol: HTTP
  #     HealthCheckPath: /
  #     HealthCheckIntervalSeconds: 10
  #     HealthCheckTimeoutSeconds: 5
  #     HealthyThresholdCount: 2
  #     UnhealthyThresholdCount: 2

  # This LogsGroup will be useful when several containers emit output:
  CloudwatchLogsGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub ${AWS::StackName}-MadlibsLogGroup
      RetentionInDays: 14

  # Cleanup function:  This function cleans out the S3 bucket on stack delete
  CustomResourceCleanupLambda:
    Type: AWS::Lambda::Function
    Properties: 
      FunctionName: !Sub ${AWS::StackName}-CleanupLambda
      Description: Cleans up S3 bucket on stack delete. 
      Role: !GetAtt CustomResourceRole.Arn
      MemorySize: 128     
      Timeout: 15         # Uploads and downloads take a bit of time.
      Runtime: python3.11
      Handler: index.lambda_handler
      Code:
        ZipFile: !Sub |
          from zipfile import ZipFile 
          import json
          import os
          import cfnresponse
          import mimetypes
          import boto3
          import urllib.request

          # Entry point:
          def lambda_handler(event, context):

              # Get the function inputs
              requestType   = event['RequestType']
              bucket      = event['ResourceProperties']['Bucket']

              if requestType == 'Delete':
                print ('Clean out bucket to enable delete... ' )
                boto3.resource('s3').Bucket(bucket).objects.all().delete()

              # Unless something blew up, we should wander into this code:
              cfnresponse.send(event, context, cfnresponse.SUCCESS, {})

              
  # This Role gives permission to our custom resource Lambda.
  CustomResourceRole:
    Type: AWS::IAM::Role
    Properties: 
      RoleName: !Sub ${AWS::StackName}-CustomResourceRole
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement: 
          Effect: Allow
          Principal:
            Service: lambda.amazonaws.com
          Action: sts:AssumeRole


  # This Policy is attached to the CustomResourceRole.
  # Basic permissions for CloudWatch Logs, plus S3.
  CustomResourcePolicy:
    Type: AWS::IAM::Policy
    Properties: 
      PolicyName: !Sub ${AWS::StackName}-CustomResourcePolicy
      PolicyDocument: 
        Version: 2012-10-17
        Statement: 
          Effect: Allow
          Action: 
            - logs:CreateLogGroup
            - logs:CreateLogStream
            - logs:PutLogEvents
            - s3:List*
            - s3:Delete*
          Resource: "*"
      Roles: 
        -  !Ref CustomResourceRole   

  # This custom resource calls our Lambda function:
  CustomResourceCleanupResource:
    Type: Custom::helper
    DependsOn: CustomResourcePolicy     # Dont try running the custom resource until the policy is set.
    Properties:
      ServiceToken: !GetAtt CustomResourceCleanupLambda.Arn
      Bucket: !Ref S3




Outputs:
  ALBDNS:
    Description: The URL to the ALB
    Value: !Sub http://${ALB.DNSName}
    Export:
      Name: !Sub ${AWS::StackName}-ALBDNS

  CodeBuildRole:
    Description:  Id of the Role to use for codebuild.
    Value: !Ref CodeBuildRole
    Export:
      Name: !Sub ${AWS::StackName}-CodeBuildRole

  CodePipelineRole:
    Description:  Id of the Role to use for Code Pipeline.
    Value: !Ref CodePipelineRole
    Export:
      Name: !Sub ${AWS::StackName}-CodePipelineRole

  CodePipelineRoleArn:
    Description:  ARN of the Role to use for Code Pipeline.
    Value: !GetAtt CodePipelineRole.Arn
    Export:
      Name: !Sub ${AWS::StackName}-CodePipelineRoleArn

  ExecutionRole:
    Description:  Id of a Role which can be used for the Task's execution role.
    Value: !Ref ExecutionRole
    Export:
      Name: !Sub ${AWS::StackName}-ExecutionRole

  TaskRole:
    Description:  Id of a Role which can be used for the Task's task role.
    Value: !Ref TaskRole
    Export:
      Name: !Sub ${AWS::StackName}-TaskRole
      
  CodePipelineBucket:
    Description:  Available S3 bucket which can be used to support CodePipeline, if desired.
    Value: !Ref S3
    Export:
      Name: !Sub ${AWS::StackName}-CodePipelineBucket

  ECSCluster:
    Description:  Name of the ECS Cluster
    Value: !Ref ECSCluster
    Export:
      Name: !Sub ${AWS::StackName}-ECSCluster

  ContainerSecurityGroup:
    Description:  A security group which can be used for most web apps.  HTTP 80 from anywhere.
    Value: !Ref ContainerSecurityGroup
    Export:
      Name: !Sub ${AWS::StackName}-ContainerSecurityGroup      

  ALB:
    Description:  An application load balancer to be used by the various services.
    Value: !Ref ALB
    Export:
      Name: !Sub ${AWS::StackName}-ALB

  ALBListener:
    Description:  Load Balancer's listener
    Value: !Ref ALBListener
    Export:
      Name: !Sub ${AWS::StackName}-ALBListener

  CloudwatchLogsGroup:
    Description:  Pre-defined logs group which can be used for output.
    Value: !Ref CloudwatchLogsGroup
    Export:
      Name: !Sub ${AWS::StackName}-CloudwatchLogsGroup

  VPC:
    Description:  VPC that was selected when creating the ALB.  
    Value: !Ref VPC
    Export:
      Name: !Sub ${AWS::StackName}-VPC    

  SubnetId1:
    Description:  First Subnets ID that was selected when creating the ALB.  Typically you'll want to associate ECS services with the same.
    Value: !Select [0, !Ref SubnetIds]
    Export:
      Name: !Sub ${AWS::StackName}-SubnetId1    

  SubnetId2:
    Description:  Second Subnet ID that was selected when creating the ALB.  Typically you'll want to associate ECS services with the same.
    Value: !Select [1, !Ref SubnetIds]
    Export:
      Name: !Sub ${AWS::StackName}-SubnetId2    